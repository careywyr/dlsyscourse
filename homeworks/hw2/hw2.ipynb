{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/careywyr/dlsyscourse/blob/main/homeworks/hw2/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-69JPTJSLlbj"
      },
      "source": [
        "# 10-714 Homework 2\n",
        "\n",
        "在本次作业中，你将使用 needle 框架实现一个神经网络库。请记得将副本保存到云盘。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px5k2q4DLlbl",
        "outputId": "6b1d51e3-9fa3-406e-e37a-0ddedbc539be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKjbPDZnM0NO",
        "outputId": "4de3d89b-ef2d-443b-d0a9-dd3f059539a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/10714\n",
            "Cloning into 'dlsyscourse'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 153 (delta 33), reused 127 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (153/153), 27.86 MiB | 11.52 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "Updating files: 100% (73/73), done.\n",
            "/content/drive/MyDrive/10714/dlsyscourse/homeworks/hw2\n"
          ]
        }
      ],
      "source": [
        "# %cd /content/drive/MyDrive/\n",
        "# !mkdir -p 10714\n",
        "# %cd /content/drive/MyDrive/10714\n",
        "# !git clone https://github.com/careywyr/dlsyscourse.git\n",
        "%cd /content/drive/MyDrive/10714/dlsyscourse/homeworks/hw2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCSfASa-Llbm"
      },
      "source": [
        "## 问题 0\n",
        "\n",
        "本次作业是在第一次作业的基础上进行的。首先，在你的作业 2 目录中，将第一次作业中的文件 `python/needle/autograd.py` 和 `python/needle/ops/ops_mathematic.py` 复制过来。\n",
        "\n",
        "***注意***：张量的默认数据类型为 `float32`。如果需要更改数据类型，可以在 `Tensor` 构造函数中设置 `dtype` 参数。例如，`Tensor([1, 2, 3], dtype='float64')` 将会创建一个数据类型为 `float64` 的张量。在本次作业中，**请确保创建的所有张量均为 `float32` 数据类型，以避免自动评分器产生问题**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Xbx855DLlbm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./python')\n",
        "sys.path.append('./apps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTR7vUdZLlbn",
        "tags": []
      },
      "source": [
        "## 问题 1\n",
        "\n",
        "在第一小题中，你需要实现几种不同的权重初始化方法。这些方法将写入文件 `python/needle/init/init_initializers.py`，该文件包含多个程序，用于通过各种随机和常量初始化方法来初始化 needle 张量。请按照现有的初始化器方法（例如在下面的函数中调用 `python/needle/init/init_basic.py` 中的 `init.rand` 或 `init.randn`），实现以下常见的初始化方法。所有函数均应返回 `fan_in` × `fan_out` 的二维张量（可以通过重塑操作扩展到其他尺寸）。\n",
        "\n",
        "### Xavier 均匀分布\n",
        "`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`\n",
        "\n",
        "根据 [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 中的方法，用均匀分布填充输入张量。填充值将从 $\\mathcal{U}(-a, a)$ 中采样，其中\n",
        "\\begin{equation}\n",
        "a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\n",
        "\\end{equation}\n",
        "\n",
        "将其余 `**kwargs` 参数传递给对应的 `init` 随机调用。\n",
        "\n",
        "##### 参数\n",
        "- `fan_in` - 输入维度\n",
        "- `fan_out` - 输出维度\n",
        "- `gain` - 可选的缩放因子\n",
        "___\n",
        "\n",
        "### Xavier 正态分布\n",
        "`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`\n",
        "\n",
        "根据 [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 中的方法，用正态分布填充输入张量。填充值将从 $\\mathcal{N}(0, \\text{std}^2)$ 中采样，其中\n",
        "\\begin{equation}\n",
        "\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\n",
        "\\end{equation}\n",
        "\n",
        "##### 参数\n",
        "- `fan_in` - 输入维度\n",
        "- `fan_out` - 输出维度\n",
        "- `gain` - 可选的缩放因子\n",
        "___\n",
        "\n",
        "### Kaiming 均匀分布\n",
        "`kaiming_uniform(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
        "\n",
        "根据 [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf) 中的方法，用均匀分布填充输入张量。填充值将从 $\\mathcal{U}(-\\text{bound}, \\text{bound})$ 中采样，其中\n",
        "\\begin{equation}\n",
        "\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan_in}}}\n",
        "\\end{equation}\n",
        "\n",
        "对于 ReLU，建议使用增益值 $\\text{gain}=\\sqrt{2}$。\n",
        "\n",
        "##### 参数\n",
        "- `fan_in` - 输入维度\n",
        "- `fan_out` - 输出维度\n",
        "- `nonlinearity` - 非线性函数\n",
        "___\n",
        "\n",
        "### Kaiming 正态分布\n",
        "`kaiming_normal(fan_in, fan_out, nonlinearity=\"relu\", **kwargs)`\n",
        "\n",
        "根据 [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf) 中的方法，用正态分布填充输入张量。填充值将从 $\\mathcal{N}(0, \\text{std}^2)$ 中采样，其中\n",
        "\\begin{equation}\n",
        "\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan_in}}}\n",
        "\\end{equation}\n",
        "\n",
        "对于 ReLU，建议使用增益值 $\\text{gain}=\\sqrt{2}$。\n",
        "\n",
        "##### 参数\n",
        "- `fan_in` - 输入维度\n",
        "- `fan_out` - 输出维度\n",
        "- `nonlinearity` - 非线性函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kXZfwqCmLlbn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 88 deselected / 4 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_init_kaiming_uniform \u001b[32mPASSED\u001b[0m\u001b[32m         [ 25%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_init_kaiming_normal \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_init_xavier_uniform \u001b[32mPASSED\u001b[0m\u001b[32m          [ 75%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_init_xavier_normal \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m88 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_init\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPsFM1pLlbo"
      },
      "source": [
        "## 问题 2\n",
        "\n",
        "在本题中，你将需要在 `python/needle/nn/nn_basic.py` 中实现更多的模块。特别是对于以下描述的模块，请在构造函数中初始化该模块的所有变量，并填写 `forward` 方法。**注意**：请务必使用刚实现的 `init` 函数来初始化参数，并确保传递 `dtype` 参数。\n",
        "___\n",
        "\n",
        "### Linear\n",
        "`needle.nn.Linear(in_features, out_features, bias=True, device=None, dtype=\"float32\")`\n",
        "\n",
        "对输入数据应用线性变换：$y = xA^T + b$。输入的形状为 $(N, H_{in})$，其中 $H_{in}=\\text{in_features}$；输出的形状为 $(N, H_{out})$，其中 $H_{out}=\\text{out_features}$。\n",
        "\n",
        "**注意需要显式地将偏置项广播到正确的形状，因为 Needle 不支持隐式广播。**\n",
        "\n",
        "**注意：对于包括此层的所有层，在初始化时应优先初始化权重张量，再初始化偏置张量，并且所有参数均应只使用 `init` 中的函数进行初始化**。这不会影响算法的正确性，主要是为了确保在本次作业的 mugrade 测试中实现的结果值符合预期。\n",
        "\n",
        "##### 参数\n",
        "- `in_features` - 每个输入样本的大小\n",
        "- `out_features` - 每个输出样本的大小\n",
        "- `bias` - 如果设置为 `False`，则该层不会学习一个加性偏置。\n",
        "\n",
        "##### 变量\n",
        "- `weight` - 可学习的权重，形状为 (`in_features`, `out_features`)。权重的值应使用 `fan_in = in_features` 的 Kaiming 均匀初始化。\n",
        "- `bias` - 可学习的偏置，形状为 (`out_features`)。偏置的值应使用 `fan_in = out_features` 的 Kaiming 均匀初始化。**注意 `fan_in` 的选择不同，这是由于它们的相对尺寸不同**。\n",
        "\n",
        "确保将所有必要的变量，例如 (`weight`, `bias`)，放入 `Parameter` 类中，以便它们能被接下来实现的优化器访问。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GmfimpH7Llbo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 84 deselected / 8 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_weight_init_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_bias_init_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 25%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 37%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m          [ 62%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 75%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 87%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_linear_backward_3 \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m84 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_linear\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGwZBZnLlbp"
      },
      "source": [
        "### ReLU\n",
        "`needle.nn.ReLU()`\n",
        "\n",
        "对输入逐元素应用修正线性单元函数：\n",
        "$ReLU(x) = \\max(0, x)$。\n",
        "\n",
        "如果你之前在实现 ReLU 的反向传播时使用了 ReLU 本身的值，请注意这种方式数值上不稳定，可能会在后续步骤中引发问题。更稳定的做法是将 ReLU 的导数表示为 $I\\{x>0\\}$，其中我们约定 $x=0$ 时的导数为 0。\n",
        "（这是一个 _次可微_ 函数。）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iepp3avHLlbp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 90 deselected / 2 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_relu_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m            [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_relu_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m90 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_relu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcEc8QMLlcA",
        "tags": []
      },
      "source": [
        "### Sequential\n",
        "`needle.nn.Sequential(*modules)`\n",
        "\n",
        "按传递给构造函数的顺序依次将多个模块应用于输入，并返回最后一个模块的输出。所有模块应保存在 `.module` 属性中：请勿重新定义诸如 `__getitem__` 等魔术方法，否则可能会与测试不兼容。\n",
        "\n",
        "##### 参数\n",
        "- `*modules` - 任意数量的 `needle.nn.Module` 类型模块\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GewFYXdULlcA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 90 deselected / 2 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_sequential_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_sequential_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m90 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_sequential\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcxgqyiCLlcA",
        "tags": []
      },
      "source": [
        "### LogSumExp\n",
        "\n",
        "`needle.ops.LogSumExp(axes)`\n",
        "\n",
        "对输入应用数值稳定的对数和指数和操作，通过先减去最大元素来实现。你需要在文件 `python/needle/ops/ops_logarithmic.py` 中实现此操作及下一个操作。\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{LogSumExp}(z) = \\log (\\sum_{i} \\exp (z_i - \\max{z})) + \\max{z}\n",
        "\\end{equation}\n",
        "\n",
        "#### 参数\n",
        "- `axes` - 指定需要求和和取最大值的轴，格式为元组。此参数的用法与 `needle.ops.Summation()` 中相同。\n",
        "\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hMI4yfmWLlcB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 82 deselected / 10 selected                               \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 10%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 20%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_3 \u001b[32mPASSED\u001b[0m\u001b[33m       [ 30%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_4 \u001b[32mPASSED\u001b[0m\u001b[33m       [ 40%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_5 \u001b[32mPASSED\u001b[0m\u001b[33m       [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_1 \u001b[32mPASSED\u001b[0m\u001b[33m      [ 60%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_2 \u001b[32mPASSED\u001b[0m\u001b[33m      [ 70%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_3 \u001b[32mPASSED\u001b[0m\u001b[33m      [ 80%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_5 \u001b[32mPASSED\u001b[0m\u001b[33m      [ 90%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_4 \u001b[32mPASSED\u001b[0m\u001b[33m      [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_forward_2\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsumexp_backward_4\n",
            "  /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2/python/needle/ops/ops_logarithmic.py:54: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "    log_sum_exp = float(log_sum_exp)\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m================ \u001b[32m10 passed\u001b[0m, \u001b[33m\u001b[1m82 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.05s\u001b[0m\u001b[33m =================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_op_logsumexp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7RO9W0rLlcB"
      },
      "source": [
        "### LogSoftmax\n",
        "\n",
        "`needle.ops.LogSoftmax(axes)`\n",
        "\n",
        "通过减去最大元素来对输入应用数值稳定的 LogSoftmax 函数。假设输入 NDArray 为二维，并在 `axis=1` 上执行 softmax 操作。\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{LogSoftmax}(z) = \\log \\left(\\frac{\\exp(z_i - \\max z)}{\\sum_{i}\\exp(z_i - \\max z)}\\right) = z - \\text{LogSumExp}(z)\n",
        "\\end{equation}\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4IOZW7ycLlcB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 89 deselected / 3 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsoftmax_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 33%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsoftmax_stable_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_op_logsoftmax_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m     [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m3 passed\u001b[0m, \u001b[33m89 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_op_logsoftmax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hREGFb7ILlcB",
        "tags": []
      },
      "source": [
        "### SoftmaxLoss\n",
        "\n",
        "`needle.nn.SoftmaxLoss()`\n",
        "\n",
        "应用以下定义的 softmax 损失（在第一次作业中已实现），输入为 logits 张量和真实标签张量（以数字列表表示，而非 one-hot 编码）。\n",
        "\n",
        "请注意，现在可以直接使用 `init.one_hot` 函数，而无需自行实现。此外，需要使用刚刚实现的数值稳定的 logsumexp 运算符来完成此操作。\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\ell_\\text{softmax}(z,y) = \\log \\sum_{i=1}^k \\exp z_i - z_y\n",
        "\\end{equation}\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AK7sR2RLLlcB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 88 deselected / 4 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m    [ 25%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m    [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_softmax_loss_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m88 deselected\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_softmax_loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ig3zfT0LlcC",
        "tags": []
      },
      "source": [
        "### LayerNorm1d\n",
        "`needle.nn.LayerNorm1d(dim, eps=1e-5, device=None, dtype=\"float32\")`\n",
        "\n",
        "对输入的小批量数据应用层归一化，如论文 [Layer Normalization](https://arxiv.org/abs/1607.06450) 所描述。\n",
        "\n",
        "\\begin{equation}\n",
        "y = w \\circ \\frac{x_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
        "\\end{equation}\n",
        "\n",
        "其中 $\\textbf{E}[x]$ 表示输入的经验均值，$\\textbf{Var}[x]$ 表示其经验方差（注意，这里使用“有偏”方差估计，即除以 $N$ 而非 $N-1$），$w$ 和 $b$ 分别表示可学习的标量权重和偏置。可以假设此层的输入为二维张量，其中批次在第一个维度，特征在第二个维度。应用权重和偏置前，可能需要对它们进行广播。\n",
        "\n",
        "##### 参数\n",
        "- `dim` - 通道数\n",
        "- `eps` - 为数值稳定性而添加到分母中的值。\n",
        "\n",
        "##### 变量\n",
        "- `weight` - 可学习的权重，大小为 `dim`，初始值为 1。\n",
        "- `bias` - 可学习的偏置，大小为 `dim`，初始值为 0。\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nbXFlRubLlcC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 85 deselected / 7 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 14%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 28%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 42%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 57%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 71%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_3 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 85%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_layernorm_backward_4 \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m7 passed\u001b[0m, \u001b[33m85 deselected\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_layernorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwJVWhhLlcC"
      },
      "source": [
        "\n",
        "### Flatten\n",
        "`needle.nn.Flatten()`\n",
        "\n",
        "接收形状为 `(B, X_0, X_1, ...)` 的张量，并将所有非批次维度展平，使输出形状为 `(B, X_0 * X_1 * ...)`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Zy_bIzNnLlcC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 83 deselected / 9 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 11%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_forward_2 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 22%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_forward_3 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 33%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_forward_4 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 44%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 55%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_backward_2 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_backward_3 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 77%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_backward_4 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 88%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_flatten_backward_5 \u001b[32mPASSED\u001b[0m\u001b[32m        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m9 passed\u001b[0m, \u001b[33m83 deselected\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_flatten\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwn0HA8LlcC"
      },
      "source": [
        "### BatchNorm1d\n",
        "`needle.nn.BatchNorm1d(dim, eps=1e-5, momentum=0.1, device=None, dtype=\"float32\")`\n",
        "\n",
        "对小批量数据应用批归一化，如论文 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) 所述。\n",
        "\n",
        "\\begin{equation}\n",
        "y = w \\circ \\frac{z_i - \\textbf{E}[x]}{((\\textbf{Var}[x]+\\epsilon)^{1/2})} + b\n",
        "\\end{equation}\n",
        "\n",
        "这里的均值和方差是指在批次维度上的均值和方差。该函数还会计算每层特征的运行均值 $\\hat{\\mu}$ 和方差 $\\hat{\\sigma}^2$，并在测试时通过这些值进行归一化：\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\frac{(x - \\hat{\\mu})}{((\\hat{\\sigma}^2_{i+1})_j+\\epsilon)^{1/2}}\n",
        "\\end{equation}\n",
        "\n",
        "在测试阶段（即调用 `model.eval()` 将 BatchNorm 层的 `training` 标志设置为 false 后），BatchNorm 使用运行均值和方差，而非批次统计值。\n",
        "\n",
        "运行均值和方差的计算公式为 $$\\hat{x_{new}} = (1 - m) \\hat{x_{old}} + mx_{observed},$$ 其中 $m$ 为动量。\n",
        "\n",
        "##### 参数\n",
        "- `dim` - 输入维度\n",
        "- `eps` - 添加到分母中的数值，用于数值稳定性。\n",
        "- `momentum` - 用于计算运行均值和方差的动量值。\n",
        "\n",
        "##### 变量\n",
        "- `weight` - 可学习的权重，大小为 `dim`，初始值为 1。\n",
        "- `bias` - 可学习的偏置，大小为 `dim`，初始值为 0。\n",
        "- `running_mean` - 测试时使用的运行均值，初始值为 0。\n",
        "- `running_var` - 测试时使用的运行（无偏）方差，初始值为 1。\n",
        "\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Dy_1ZygZLlcD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 84 deselected / 8 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_check_model_eval_switches_training_flag_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [ 25%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_forward_affine_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m      [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_backward_affine_1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_running_mean_1 \u001b[32mPASSED\u001b[0m\u001b[32m  [ 75%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_running_var_1 \u001b[32mPASSED\u001b[0m\u001b[32m   [ 87%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_batchnorm_running_grad_1 \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m84 deselected\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_batchnorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3YASgoDLlcD"
      },
      "source": [
        "### Dropout\n",
        "`needle.nn.Dropout(p = 0.5)`\n",
        "\n",
        "在训练期间，Dropout 模块通过从伯努利分布采样，以概率 `p` 随机将输入张量的部分元素置零。这种方法已被证明是一种有效的正则化技术，能够防止神经元的协同适应，详见论文 [Improving neural networks by preventing co-adaption of feature detectors](https://arxiv.org/abs/1207.0580)。在评估期间，模块不执行 Dropout 操作，仅返回输入。\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{z}_{i+1} = \\sigma_i (W_i^T z_i + b_i) \\\\\n",
        "(z_{i+1})_j =\n",
        "    \\begin{cases}\n",
        "    (\\hat{z}_{i+1})_j /(1-p) & \\text{with probability } 1-p \\\\\n",
        "    0 & \\text{with probability } p \\\\\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "**重要**：如果 Dropout 模块的 `training` 标志设置为 False，则不应“丢弃”任何权重，即 Dropout 仅在训练期间启用，不适用于评估期间。请注意，`training` 是 `nn.Module` 中的一个标志。\n",
        "\n",
        "##### 参数\n",
        "- `p` - 元素被置零的概率。\n",
        "\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xUwEemClLlcD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 90 deselected / 2 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_dropout_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_dropout_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m90 deselected\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_dropout\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA709XeMLlcD",
        "tags": []
      },
      "source": [
        "### Residual\n",
        "`needle.nn.Residual(fn: Module)`\n",
        "\n",
        "对输入张量 $x$ 应用残差或跳跃连接，给定模块 $\\mathcal{F}$，返回 $\\mathcal{F}(x) + x$。\n",
        "\n",
        "##### 参数\n",
        "- `fn` - `needle.nn.Module` 类型的模块\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HnnfnnUOLlcD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 90 deselected / 2 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_residual_forward_1 \u001b[32mPASSED\u001b[0m\u001b[32m        [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_nn_residual_backward_1 \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m90 deselected\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_nn_residual\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJhDLU-ILlcJ",
        "tags": []
      },
      "source": [
        "## 问题 3\n",
        "\n",
        "在 `python/needle/optim.py` 中实现以下优化器的 `step` 函数。确保优化器**不会**在原地修改张量的梯度。\n",
        "\n",
        "我们提供了一些测试，以确保不会出现过高的内存消耗。如果没有在合适的位置使用 `.data` 或 `.detach()`，则会生成越来越大的计算图（不仅在优化器中，在前面的模块中也是如此）。对于包含字符串 `memory_check` 的测试，你可以自行决定是否忽略。\n",
        "\n",
        "___\n",
        "\n",
        "### SGD\n",
        "`needle.optim.SGD(params, lr=0.01, momentum=0.0, weight_decay=0.0)`\n",
        "\n",
        "实现随机梯度下降（可选动量 $\\beta$）：\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "    u_{t+1} &= \\beta u_t + (1-\\beta) \\nabla_\\theta f(\\theta_t) \\\\\n",
        "    \\theta_{t+1} &= \\theta_t - \\alpha u_{t+1}\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "##### 参数\n",
        "- `params` - `needle.nn.Parameter` 类型的参数迭代器，用于优化\n",
        "- `lr` (*float*) - 学习率\n",
        "- `momentum` (*float*) - 动量因子\n",
        "- `weight_decay` (*float*) - 权重衰减（L2 正则化项）\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "J_1ow-cGLlcJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 86 deselected / 6 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_vanilla_1 \u001b[31mFAILED\u001b[0m\u001b[31m          [ 16%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_momentum_1 \u001b[31mFAILED\u001b[0m\u001b[31m         [ 33%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_weight_decay_1 \u001b[31mFAILED\u001b[0m\u001b[31m     [ 50%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_momentum_weight_decay_1 \u001b[31mFAILED\u001b[0m\u001b[31m [ 66%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_layernorm_residual_1 \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_sgd_z_memory_check_1 \u001b[32mPASSED\u001b[0m\u001b[31m   [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m___________________________ test_optim_sgd_vanilla_1 ___________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_sgd_vanilla_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.SGD,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                momentum=\u001b[94m0.0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.207009\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1791: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x1050c2e80>, array(3.2115598, dtype=float32), array(3.207009))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.00455077\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.00141901\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(3.21156, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(3.207009)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m__________________________ test_optim_sgd_momentum_1 ___________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_sgd_momentum_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.SGD,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                momentum=\u001b[94m0.9\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.311805\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1807: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x10513be20>, array(2.8634734, dtype=float32), array(3.311805))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.44833158\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.13537379\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(2.863473, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(3.311805)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m________________________ test_optim_sgd_weight_decay_1 _________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_sgd_weight_decay_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.SGD,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                momentum=\u001b[94m0.0\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.202637\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1823: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x105144900>, array(3.2034044, dtype=float32), array(3.202637))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.00076743\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.00023962\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(3.203404, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(3.202637)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m____________________ test_optim_sgd_momentum_weight_decay_1 ____________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_sgd_momentum_weight_decay_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.SGD,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                momentum=\u001b[94m0.9\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.306993\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1840: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x105144fe0>, array(2.863359, dtype=float32), array(3.306993))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.44363403\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.13415028\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(2.863359, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(3.306993)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m_____________________ test_optim_sgd_layernorm_residual_1 ______________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_sgd_layernorm_residual_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        nn.LayerNorm1d(\u001b[94m8\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "                    nn.ReLU(),\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Residual(nn.Linear(\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Linear(\u001b[94m8\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "                ),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.SGD,\u001b[90m\u001b[39;49;00m\n",
            "                epochs=\u001b[94m3\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m2.852236\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1858: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x1050c31a0>, array(2.929468, dtype=float32), array(2.852236))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.07723192\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.02707767\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(2.929468, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(2.852236)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 8), grad_rhs.shape=(8, 8)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 8), rhs.shape=(32, 8), grad.shape=(32, 8)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 8)\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_sgd_vanilla_1\u001b[0m - AssertionError: \n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_sgd_momentum_1\u001b[0m - AssertionError: \n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_sgd_weight_decay_1\u001b[0m - AssertionError: \n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_sgd_momentum_weight_decay_1\u001b[0m - AssertionError: \n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_sgd_layernorm_residual_1\u001b[0m - AssertionError: \n",
            "\u001b[31m================== \u001b[31m\u001b[1m5 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m86 deselected\u001b[0m\u001b[31m in 0.21s\u001b[0m\u001b[31m ==================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_optim_sgd\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMbx8NgcLlcJ",
        "tags": []
      },
      "source": [
        "### Adam\n",
        "`needle.optim.Adam(params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0)`\n",
        "\n",
        "实现 Adam 算法，详见论文 [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)。\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "u_{t+1} &= \\beta_1 u_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t) \\\\\n",
        "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2) (\\nabla_\\theta f(\\theta_t))^2 \\\\\n",
        "\\hat{u}_{t+1} &= u_{t+1} / (1 - \\beta_1^t) \\quad \\text{(偏差校正)} \\\\\n",
        "\\hat{v}_{t+1} &= v_{t+1} / (1 - \\beta_2^t) \\quad \\text{(偏差校正)}\\\\\n",
        "\\theta_{t+1} &= \\theta_t - \\alpha \\hat{u_{t+1}}/(\\hat{v}_{t+1}^{1/2}+\\epsilon)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "**重要**：注意是否正确应用了偏差校正。\n",
        "\n",
        "##### 参数\n",
        "- `params` - `needle.nn.Parameter` 类型的参数迭代器，用于优化\n",
        "- `lr` (*float*) - 学习率\n",
        "- `beta1` (*float*) - 用于计算梯度运行平均的系数\n",
        "- `beta2` (*float*) - 用于计算梯度平方运行平均的系数\n",
        "- `eps` (*float*) - 为提升数值稳定性而添加到分母的值\n",
        "- `weight_decay` (*float*) - 权重衰减（L2 正则项）\n",
        "\n",
        "**提示**：为减少内存占用，尝试使用 `.data` 或 `.detach()`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2_b3FTtpLlcJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 85 deselected / 7 selected                                \u001b[0m\n",
            "\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_1 \u001b[31mFAILED\u001b[0m\u001b[31m                 [ 14%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_weight_decay_1 \u001b[31mFAILED\u001b[0m\u001b[31m    [ 28%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_batchnorm_1 \u001b[31mFAILED\u001b[0m\u001b[31m       [ 42%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_batchnorm_eval_mode_1 \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_layernorm_1 \u001b[31mFAILED\u001b[0m\u001b[31m       [ 71%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_weight_decay_bias_correction_1 \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
            "tests/hw2/test_nn_and_optim.py::test_optim_adam_z_memory_check_1 \u001b[31mFAILED\u001b[0m\u001b[31m  [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_optim_adam_1 _______________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.703999\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1889: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:215: in learn_model_1d\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.07782817  0.27500503  0.1420667  ... -0.27674125  0.0648575\n",
            "  -0.20076195]\n",
            " [-0.26735042  0.2738851....2219162   0.01885353\n",
            "  -0.0534426 ]\n",
            " [-0.09448274  0.24384693 -0.29282218 ...  0.13544534 -0.26574798\n",
            "   0.12827249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m________________________ test_optim_adam_weight_decay_1 ________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_weight_decay_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.705134\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1904: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:215: in learn_model_1d\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.07782817  0.27500503  0.14106671 ... -0.27674125  0.0648575\n",
            "  -0.20076195]\n",
            " [-0.26735042  0.2738851....2219162   0.01885353\n",
            "  -0.0534426 ]\n",
            " [-0.09448274  0.24384693 -0.29182219 ...  0.13544534 -0.26574798\n",
            "   0.12827249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m_________________________ test_optim_adam_batchnorm_1 __________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_batchnorm_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.BatchNorm1d(\u001b[94m32\u001b[39;49;00m), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                ),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.296256\u001b[39;49;00m, dtype=np.float32),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1920: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:215: in learn_model_1d\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.07582817  0.27500503  0.14106677 ... -0.27674125  0.0648575\n",
            "  -0.20076195]\n",
            " [-0.26735042  0.2738851....2219162   0.01885353\n",
            "  -0.0534426 ]\n",
            " [-0.09448274  0.24584692 -0.29182222 ...  0.13544534 -0.26574798\n",
            "   0.12627249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseMul.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m____________________ test_optim_adam_batchnorm_eval_mode_1 _____________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_batchnorm_eval_mode_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d_eval(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.BatchNorm1d(\u001b[94m32\u001b[39;49;00m), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                ),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.192054\u001b[39;49;00m, dtype=np.float32),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1938: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:247: in learn_model_1d_eval\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.07582817  0.27500503  0.14106677 ... -0.27674125  0.0648575\n",
            "  -0.20076195]\n",
            " [-0.26735042  0.2738851....2219162   0.01885353\n",
            "  -0.0534426 ]\n",
            " [-0.09448274  0.24584692 -0.29182222 ...  0.13544534 -0.26574798\n",
            "   0.12627249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseMul.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m_________________________ test_optim_adam_layernorm_1 __________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_layernorm_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(\u001b[90m\u001b[39;49;00m\n",
            "                    nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.LayerNorm1d(\u001b[94m32\u001b[39;49;00m), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                ),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m2.82192\u001b[39;49;00m, dtype=np.float32),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1956: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:215: in learn_model_1d\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.08682817  0.26600504  0.13206677 ... -0.26774126  0.07585742\n",
            "  -0.19176195]\n",
            " [-0.27635042  0.284885....21091621  0.00985354\n",
            "  -0.0444426 ]\n",
            " [-0.10348274  0.25484692 -0.28282222 ...  0.14644534 -0.25474799\n",
            "   0.13727249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseMul.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m________________ test_optim_adam_weight_decay_bias_correction_1 ________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_weight_decay_bias_correction_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            ">           learn_model_1d(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m64\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94m16\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mlambda\u001b[39;49;00m z: nn.Sequential(nn.Linear(\u001b[94m64\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m), nn.ReLU(), nn.Linear(\u001b[94m32\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
            "                ndl.optim.Adam,\u001b[90m\u001b[39;49;00m\n",
            "                lr=\u001b[94m0.001\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                weight_decay=\u001b[94m0.01\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            ),\u001b[90m\u001b[39;49;00m\n",
            "            np.array(\u001b[94m3.705134\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
            "            rtol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "            atol=\u001b[94m1e-5\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1974: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:215: in learn_model_1d\n",
            "    \u001b[0mopt.step()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mpython/needle/optim.py\u001b[0m:97: in step\n",
            "    \u001b[0mparam.data = param.data - \u001b[96mself\u001b[39;49;00m.lr * m_hat / (v_hat ** \u001b[94m0.5\u001b[39;49;00m + \u001b[96mself\u001b[39;49;00m.eps)\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = needle.Tensor([[-0.07682817  0.27600503  0.1420667  ... -0.27774125  0.0658575\n",
            "  -0.20176195]\n",
            " [-0.26635042  0.2748851....2209162   0.01985353\n",
            "  -0.0544426 ]\n",
            " [-0.09348274  0.24484693 -0.29282218 ...  0.13644534 -0.26474798\n",
            "   0.12727249]])\n",
            "value = needle.Tensor([[-0.07782817  0.27500503  0.14106671 ... -0.27674125  0.0648575\n",
            "  -0.20076195]\n",
            " [-0.26735042  0.2738851....2219162   0.01885353\n",
            "  -0.0534426 ]\n",
            " [-0.09448274  0.24384693 -0.29182219 ...  0.13544534 -0.26574798\n",
            "   0.12827249]])\n",
            "\n",
            "    \u001b[0m\u001b[37m@data\u001b[39;49;00m.setter\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mdata\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, value):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(value, Tensor)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m value.dtype == \u001b[96mself\u001b[39;49;00m.dtype, \u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m % (\u001b[90m\u001b[39;49;00m\n",
            "            value.dtype,\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[96mself\u001b[39;49;00m.dtype,\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: float64 float32\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:264: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "EWiseAdd.gradient, lhs.shape:(32,), rhs.shape=(32,), grad.shape=(32,)\n",
            "EWiseMul.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 16), rhs.shape=(32, 16), grad.shape=(32, 16)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 32), grad_rhs.shape=(32, 16)\n",
            "EWiseAdd.gradient, lhs.shape:(32, 32), rhs.shape=(32, 32), grad.shape=(32, 32)\n",
            "MatMul.gradient result, grad_lhs.shape:(32, 64), grad_rhs.shape=(64, 32)\n",
            "\u001b[31m\u001b[1m_______________________ test_optim_adam_z_memory_check_1 _______________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_optim_adam_z_memory_check_1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
            "            global_tensor_count(), np.array(\u001b[94m1132\u001b[39;49;00m), rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1000\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        )\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/hw2/test_nn_and_optim.py\u001b[0m:1991: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "args = (<function assert_allclose.<locals>.compare at 0x1045d7060>, array(106), array(1132))\n",
            "kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1000', 'strict': False, ...}\n",
            "\n",
            "    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",
            ">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1000\u001b[0m\n",
            "\u001b[1m\u001b[31mE           \u001b[0m\n",
            "\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max absolute difference among violations: 1026\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Max relative difference among violations: 0.90636042\u001b[0m\n",
            "\u001b[1m\u001b[31mE            ACTUAL: array(106)\u001b[0m\n",
            "\u001b[1m\u001b[31mE            DESIRED: array(1132)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/opt/anaconda3/envs/dlsys/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_weight_decay_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_batchnorm_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_batchnorm_eval_mode_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_layernorm_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_weight_decay_bias_correction_1\u001b[0m - AssertionError: float64 float32\n",
            "\u001b[31mFAILED\u001b[0m tests/hw2/test_nn_and_optim.py::\u001b[1mtest_optim_adam_z_memory_check_1\u001b[0m - AssertionError: \n",
            "\u001b[31m======================= \u001b[31m\u001b[1m7 failed\u001b[0m, \u001b[33m85 deselected\u001b[0m\u001b[31m in 0.17s\u001b[0m\u001b[31m =======================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_optim_adam\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa5zJDGqLlcK"
      },
      "source": [
        "## 问题 4\n",
        "\n",
        "本题中，你将实现两个数据基础类：`needle.data.DataLoader` 和 `needle.data.Dataset`。`Dataset` 用于存储样本及其对应标签，而 `DataLoader` 则将 `Dataset` 包装为一个可迭代对象，便于访问样本。\n",
        "\n",
        "本题要求你在 `python/needle/data` 目录下进行实现。\n",
        "\n",
        "### 转换\n",
        "\n",
        "首先，我们将实现一些图像处理中的常用转换，目前包括水平翻转和随机裁剪。在 `needle/data/data_transforms.py` 中填写以下函数。\n",
        "___\n",
        "\n",
        "#### RandomFlipHorizontal\n",
        "`needle.data.RandomFlipHorizontal(p = 0.5)`\n",
        "\n",
        "以概率 `p` 将图像进行水平翻转。\n",
        "\n",
        "##### 参数\n",
        "- `p` (*float*) - 图像水平翻转的概率。\n",
        "___\n",
        "\n",
        "#### RandomCrop\n",
        "`needle.data.RandomCrop(padding=3)`\n",
        "\n",
        "在图像的每一边添加填充后，在随机位置将图像裁剪回原始大小。输出图像大小与原图相同。\n",
        "\n",
        "##### 参数\n",
        "- `padding` (*int*) - 图像每个边缘的填充大小。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfUE6jdYLlcK"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -v -k \"flip_horizontal\"\n",
        "!python3 -m pytest -v -k \"random_crop\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C7LQB-eLlcK"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "每个 `Dataset` 子类必须实现 `__init__`、`__len__` 和 `__getitem__` 三个函数。`__init__` 函数初始化图像、标签和转换操作。`__len__` 返回数据集中样本数量，`__getitem__` 则在给定索引 `idx` 处获取数据集样本，对图像应用转换函数（如果适用），并将图像和标签转换为 numpy 数组（数据将在其他部分转换为 Tensor）。`__getitem__` 和 `__next__` 的输出应为大小为 (样本数, 特征维度1, 特征维度2, ...) 的 NDArray。\n",
        "\n",
        "在 `needle/data/datasets/mnist_dataset.py` 中的 `MNISTDataset` 类中实现这些函数。`__init__` 函数可以使用上次作业中的 `parse_mnist` 方法。\n",
        "\n",
        "### MNISTDataset\n",
        "`needle.data.MNISTDataset(image_filesname, label_filesname, transforms)`\n",
        "\n",
        "##### 参数\n",
        "- `image_filesname` - 图像文件的路径\n",
        "- `label_filesname` - 标签文件的路径\n",
        "- `transforms` - 可选的转换列表，用于数据转换"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TR79XTEeLlcK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform darwin -- Python 3.12.7, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/dlsys/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /Users/leafw/Documents/workspace/dlsyscourse/homeworks/hw2\n",
            "collected 92 items / 92 deselected / 0 selected                                \u001b[0m\n",
            "\n",
            "\u001b[33m============================ \u001b[33m\u001b[1m92 deselected\u001b[0m\u001b[33m in 0.04s\u001b[0m\u001b[33m ============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pytest -v -k \"test_mnist_dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w2yj2IULlcK"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "`needle/data/data_basic.py` 中的 Dataloader 类提供了一个接口，用于将样本组装为适合 SGD 训练的小批量数据，基于 Dataset 对象。要构建标准的 Dataloader 接口（允许用户遍历数据集中的所有小批量），需要在类中实现 `__iter__()` 和 `__next__()` 方法：`__iter__()` 在迭代开始时调用，而 `__next__()` 则获取下一个小批量。注意，`next` 后续调用需返回后续小批量，因此不是纯函数。\n",
        "\n",
        "### Dataloader\n",
        "`needle.data.Dataloader(dataset: Dataset, batch_size: Optional[int] = 1, shuffle: bool = False)`\n",
        "\n",
        "将数据集与采样器组合，并提供对数据集的可迭代访问。\n",
        "\n",
        "##### 参数\n",
        "- `dataset` - `needle.data.Dataset` - 数据集对象\n",
        "- `batch_size` - `int` - 批次大小\n",
        "- `shuffle` - `bool` - 每个 epoch 是否打乱数据，默认值为 ``False``。\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmiTB10HLlcL"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -v -k \"test_dataloader\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75IvCnIPLlcL"
      },
      "source": [
        "## 问题 5\n",
        "\n",
        "现在你已经实现了所有必要的神经网络库组件，我们可以构建并训练一个 MLP ResNet。在本题中，请在 `apps/mlp_resnet.py` 文件中实现 `ResidualBlock` 和 `MLPResNet` 函数。\n",
        "\n",
        "### ResidualBlock\n",
        "`ResidualBlock(dim, hidden_dim, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
        "\n",
        "实现如下所示的残差块：\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.github.com/dlsyscourse/hw2/blob/main/figures/residualblock.png\" alt=\"Residual Block\"/>\n",
        "</p>\n",
        "\n",
        "**注意**：如果图像未显示，请查看 `figures` 目录中的图像。\n",
        "\n",
        "该模块的第一个线性层的 `in_features=dim`，`out_features=hidden_dim`，最后一个线性层的 `out_features=dim`。返回类型为 `nn.Module`。\n",
        "\n",
        "##### 参数\n",
        "- `dim` (*int*) - 输入维度\n",
        "- `hidden_dim` (*int*) - 隐藏层维度\n",
        "- `norm` (*nn.Module*) - 归一化方法\n",
        "- `drop_prob` (*float*) - dropout 概率\n",
        "\n",
        "___\n",
        "\n",
        "### MLPResNet\n",
        "`MLPResNet(dim, hidden_dim=100, num_blocks=3, num_classes=10, norm=nn.BatchNorm1d, drop_prob=0.1)`\n",
        "\n",
        "实现如下所示的 MLP ResNet：\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.github.com/dlsyscourse/hw2/blob/main/figures/mlp_resnet.png\" alt=\"MLP Resnet\"/>\n",
        "</p>\n",
        "\n",
        "第一个线性层的 `in_features=dim`，`out_features=hidden_dim`。每个 ResidualBlock 的 `dim=hidden_dim`，`hidden_dim=hidden_dim//2`。返回网络类型为 `nn.Module`。\n",
        "\n",
        "##### 参数\n",
        "- `dim` (*int*) - 输入维度\n",
        "- `hidden_dim` (*int*) - 隐藏层维度\n",
        "- `num_blocks` (*int*) - 残差块的数量\n",
        "- `num_classes` (*int*) - 类别数\n",
        "- `norm` (*nn.Module*) - 归一化方法\n",
        "- `drop_prob` (*float*) - dropout 概率 (0.1)\n",
        "\n",
        "**注意**：模块应按 ResNet 中的执行顺序进行初始化。\n",
        "___\n",
        "\n",
        "完成模型架构后，使用新的神经网络库组件来训练网络。具体来说，实现 `epoch` 和 `train_mnist` 函数。\n",
        "\n",
        "### Epoch\n",
        "\n",
        "`epoch(dataloader, model, opt=None)`\n",
        "\n",
        "执行一个训练或评估 epoch，遍历整个训练数据集一次（如之前作业中的 `nn_epoch`）。返回平均错误率（*float*）和所有样本的平均损失（*float*）。若 `opt` 存在，则在函数开始时将模型设为 `training` 模式；若 `opt` 为 `None`，则设为 `eval` 模式。请使用 `.train()` 和 `.eval()` 设置模式，而不是直接修改 training 属性。\n",
        "\n",
        "##### 参数\n",
        "- `dataloader` (*`needle.data.DataLoader`*) - 返回训练样本的数据加载器\n",
        "- `model` (*`needle.nn.Module`*) - 神经网络\n",
        "- `opt` (*`needle.optim.Optimizer`*) - 优化器实例，或 `None`\n",
        "\n",
        "___\n",
        "\n",
        "### 训练 MNIST\n",
        "\n",
        "`train_mnist(batch_size=100, epochs=10, optimizer=ndl.optim.Adam, lr=0.001, weight_decay=0.001, hidden_dim=100, data_dir=\"data\")`\n",
        "\n",
        "初始化用于 MNIST 数据的训练数据加载器（`shuffle=True`）和测试数据加载器，并使用指定优化器（若 `opt` 存在）和 softmax 损失函数进行训练。返回一个元组，包括最后一个 epoch 的训练错误率、训练损失、测试错误率和测试损失。若参数未指定，请使用默认值。\n",
        "\n",
        "##### 参数\n",
        "- `batch_size` (*int*) - 训练和测试数据加载器的批次大小\n",
        "- `epochs` (*int*) - 训练 epoch 数\n",
        "- `optimizer` (*`needle.optim.Optimizer` 类型*) - 使用的优化器类型\n",
        "- `lr` (*float*) - 学习率\n",
        "- `weight_decay` (*float*) - 权重衰减\n",
        "- `hidden_dim` (*int*) - MLPResNet 的隐藏层维度\n",
        "- `data_dir` (*str*) - 包含 MNIST 图像/标签的目录\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMlmGIFILlcL"
      },
      "outputs": [],
      "source": [
        "!python3 -m pytest -v -k \"test_mlp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP5zYxMALlcM"
      },
      "source": [
        "我们鼓励你尝试使用 `mlp_resnet.py` 训练脚本。你可以观察不同初始化方法在 Linear 层中的效果、提高 dropout 概率，或通过 Dataset 的 `transforms=` 参数添加转换（如随机裁剪）。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "dlsys",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
